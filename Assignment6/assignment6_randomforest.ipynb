{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9897a65",
   "metadata": {},
   "source": [
    "### Project 1: predict the function of the proteins on InterPROscan dataset using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e19cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import desc, avg, collect_list, flatten, split, countDistinct, explode, asc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, isnan, when, count\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1225fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/commons/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/08 20:38:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/08 20:38:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/07/08 20:38:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memory', '180g'),\n",
       " ('spark.driver.port', '42877'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.executor.memory', '180g'),\n",
       " ('spark.app.id', 'local-1657305504486'),\n",
       " ('spark.app.startTime', '1657305503310'),\n",
       " ('spark.master', 'local[16]'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'assemblix2019.bin.bioinf.nl'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '128g'),\n",
    "                                   ('spark.master', 'local[16]'),\n",
    "                                   ('spark.driver.memory', '128g')])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88224972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file = '/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "df = SQLContext(sc).read.csv(file, sep=r'\\t', header=False, inferSchema= True)\n",
    "#rename the column name\n",
    "new_names = ['Protein_accession', 'MD5', 'Seq_len', 'Analysis',\n",
    "             'Signature_accession', 'Signature_description',\n",
    "             'Start', 'Stop', 'Score', 'Status', 'Date', 'InterPro_annotations',\n",
    "             'InterPro_discription', 'GO_annotations', 'Pathways']\n",
    "df = df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a4ef416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+--------+-------------------+---------------------+-----+----+-------+------+----------+--------------------+--------------------+--------------+--------+\n",
      "|   Protein_accession|                 MD5|Seq_len|Analysis|Signature_accession|Signature_description|Start|Stop|  Score|Status|      Date|InterPro_annotations|InterPro_discription|GO_annotations|Pathways|\n",
      "+--------------------+--------------------+-------+--------+-------------------+---------------------+-----+----+-------+------+----------+--------------------+--------------------+--------------+--------+\n",
      "|gi|29898682|gb|AA...|92d1264e347e14924...|    547| TIGRFAM|          TIGR03882| cyclo_dehyd_2: ba...|    2| 131|1.6E-21|     T|25-04-2022|           IPR022291|Bacteriocin biosy...|             -|       -|\n",
      "+--------------------+--------------------+-------+--------+-------------------+---------------------+-----+----+-------+------+----------+--------------------+--------------------+--------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a80f1d",
   "metadata": {},
   "source": [
    "###### first drop every columns that we don't need it in next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59cd318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+----+--------------------+\n",
      "|   Protein_accession|Seq_len|Start|Stop|InterPro_annotations|\n",
      "+--------------------+-------+-----+----+--------------------+\n",
      "|gi|29898682|gb|AA...|    547|    2| 131|           IPR022291|\n",
      "|gi|29898682|gb|AA...|    547|  161| 547|           IPR027624|\n",
      "|gi|29898682|gb|AA...|    547|  159| 547|           IPR003776|\n",
      "+--------------------+-------+-----+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"MD5\", \"Analysis\", \"Signature_accession\", \"Signature_description\", \"Score\", \"Status\", \"Date\", \"InterPro_discription\", \"GO_annotations\", \"Pathways\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb383bd5",
   "metadata": {},
   "source": [
    "##### filter out the rows consisting of no annotation.\n",
    "##### number of Distinct \"InterPro_annotations\" in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4412dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9703"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(df.InterPro_annotations!=\"-\")\n",
    "df.select(countDistinct(\"InterPro_annotations\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3534cf5",
   "metadata": {},
   "source": [
    "###### check whether there is any null records in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "954743aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================================> (82 + 2) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-----+----+--------------------+\n",
      "|Protein_accession|Seq_len|Start|Stop|InterPro_annotations|\n",
      "+-----------------+-------+-----+----+--------------------+\n",
      "|                0|      0|    0|   0|                   0|\n",
      "+-----------------+-------+-----+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7350077",
   "metadata": {},
   "source": [
    "###### how many recordes in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a9f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1921817"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9599950",
   "metadata": {},
   "source": [
    "###### number of distict protein in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01487e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "332775"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(countDistinct(\"Protein_accession\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcd916",
   "metadata": {},
   "source": [
    "###### adding \"size\" column that is the percentage of annotation size to protein size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1b7e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+----+--------------------+------------------+\n",
      "|   Protein_accession|Seq_len|Start|Stop|InterPro_annotations|              size|\n",
      "+--------------------+-------+-----+----+--------------------+------------------+\n",
      "|gi|29898682|gb|AA...|    547|    2| 131|           IPR022291|23.583180987202926|\n",
      "|gi|29898682|gb|AA...|    547|  161| 547|           IPR027624| 70.56672760511883|\n",
      "+--------------------+-------+-----+----+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('size', (((df['Stop'] - df['Start']) / df.Seq_len) * 100))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41853842",
   "metadata": {},
   "source": [
    "###### create a dataframe for large annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f86d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4980"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_df = df.where (df.size >= 90)\n",
    "large_df.select(countDistinct(\"InterPro_annotations\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87533986",
   "metadata": {},
   "source": [
    "###### according to below output, there are more than on large Interpro_annotation for one protein;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4a0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+----+--------------------+-----------------+\n",
      "|   Protein_accession|Seq_len|Start|Stop|InterPro_annotations|             size|\n",
      "+--------------------+-------+-----+----+--------------------+-----------------+\n",
      "|gi|10172613|dbj|B...|    449|    1| 448|           IPR001957|99.55456570155901|\n",
      "|gi|10172613|dbj|B...|    449|    5| 448|           IPR001957|98.66369710467706|\n",
      "|gi|10172613|dbj|B...|    449|    8| 447|           IPR001957| 97.7728285077951|\n",
      "|gi|10172614|dbj|B...|    380|   17| 375|           IPR001001|94.21052631578948|\n",
      "|gi|10172614|dbj|B...|    380|    1| 378|           IPR001001|99.21052631578947|\n",
      "|gi|10172614|dbj|B...|    380|    1| 379|           IPR001001|99.47368421052632|\n",
      "|gi|10172614|dbj|B...|    380|    1| 380|           IPR001001|99.73684210526315|\n",
      "|gi|10172614|dbj|B...|    380|    1| 379|           IPR001001|99.47368421052632|\n",
      "|gi|10172615|dbj|B...|     73|    1|  68|           IPR036986|91.78082191780823|\n",
      "|gi|10172616|dbj|B...|    371|    1| 368|           IPR001238|98.92183288409704|\n",
      "|gi|10172616|dbj|B...|    371|    1| 371|           IPR001238|99.73045822102425|\n",
      "|gi|10172616|dbj|B...|    371|    1| 364|           IPR001238|97.84366576819407|\n",
      "|gi|10172616|dbj|B...|    371|    1| 350|           IPR027417|94.07008086253369|\n",
      "|gi|10172616|dbj|B...|    371|    6| 344|           IPR027417|91.10512129380054|\n",
      "|gi|10172616|dbj|B...|    371|    2| 347|           IPR003395|92.99191374663073|\n",
      "|gi|10172618|dbj|B...|    637|    8| 637|           IPR011557|98.74411302982732|\n",
      "|gi|10172618|dbj|B...|    637|   37| 630|           IPR001241|93.09262166405023|\n",
      "|gi|10172618|dbj|B...|    637|    5| 637|           IPR011557|99.21507064364206|\n",
      "|gi|10172619|dbj|B...|    833|    6| 806|           IPR005743|96.03841536614645|\n",
      "|gi|10172625|dbj|B...|    250|   15| 250|           IPR008841|             94.0|\n",
      "+--------------------+-------+-----+----+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "large_df.select(\"*\").sort(\"Protein_accession\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acfa55d",
   "metadata": {},
   "source": [
    "###### I pick only the largest one for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025a1d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==========>                                           (39 + 17) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+----+--------------------+-----------------+\n",
      "|   Protein_accession|Seq_len|Start|Stop|InterPro_annotations|             size|\n",
      "+--------------------+-------+-----+----+--------------------+-----------------+\n",
      "|gi|10172613|dbj|B...|    449|    1| 448|           IPR001957|99.55456570155901|\n",
      "|gi|10172614|dbj|B...|    380|    1| 380|           IPR001001|99.73684210526315|\n",
      "|gi|10172615|dbj|B...|     73|    1|  68|           IPR036986|91.78082191780823|\n",
      "|gi|10172616|dbj|B...|    371|    1| 371|           IPR001238|99.73045822102425|\n",
      "|gi|10172618|dbj|B...|    637|    5| 637|           IPR011557|99.21507064364206|\n",
      "|gi|10172619|dbj|B...|    833|    6| 806|           IPR005743|96.03841536614645|\n",
      "|gi|10172625|dbj|B...|    250|   15| 250|           IPR008841|             94.0|\n",
      "|gi|10172631|dbj|B...|    317|    9| 312|           IPR026988|95.58359621451105|\n",
      "|gi|10172632|dbj|B...|    485|    1| 485|           IPR005990|99.79381443298969|\n",
      "|gi|10172634|dbj|B...|    298|    1| 294|           IPR001852| 98.3221476510067|\n",
      "+--------------------+-------+-----+----+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:===============================>                     (118 + 19) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "windowpro = Window.partitionBy(\"Protein_accession\").orderBy(col(\"size\").desc())\n",
    "largdff = large_df.withColumn(\"row\",row_number().over(windowpro)).filter(col(\"row\") == 1).drop(\"row\")\n",
    "largdff.sort(\"Protein_accession\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e969b3",
   "metadata": {},
   "source": [
    "###### to make sure that there are only one large annotation for every protein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f1902f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Protein_accession|count|\n",
      "+-----------------+-----+\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "largdff.groupBy(\"Protein_accession\").count().filter(\"count > 1\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cbdf7",
   "metadata": {},
   "source": [
    "###### make a list of distinct Protein that have large feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c7cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = largdff.select(\"Protein_accession\").collect()\n",
    "all_protein = [i[0] for i in ap]\n",
    "len(all_protein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22d0f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208741"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_protein)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c6a5b",
   "metadata": {},
   "source": [
    "###### I drop the columns that I don't need them any more; I use largdff for marging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8fd0742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:====================================================>   (79 + 5) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|   Protein_accession|InterPro_annotations|\n",
      "+--------------------+--------------------+\n",
      "|gi|10172751|dbj|B...|           IPR036394|\n",
      "|gi|10172776|dbj|B...|           IPR027417|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 33:======================================================> (82 + 2) / 84]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "largdff = largdff.drop(\"Seq_len\", \"Start\", \"Stop\", \"size\")\n",
    "largdff.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a7ee06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "small_df = df.filter(df.Protein_accession.isin(all_protein)).where(df.size < 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3449699",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 20:46:29 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "[Stage 35:>                                                       (0 + 16) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[660.201s][warning][codecache] CodeHeap 'non-profiled nmethods' is full. Compiler has been disabled.\n",
      "[660.201s][warning][codecache] Try increasing the code heap size using -XX:NonProfiledCodeHeapSize=\n",
      "CodeHeap 'non-profiled nmethods': size=118312Kb used=116751Kb max_used=117898Kb free=1560Kb\n",
      " bounds [0x00007f1a54494000, 0x00007f1a5b81e000, 0x00007f1a5b81e000]\n",
      "CodeHeap 'profiled nmethods': size=118308Kb used=105058Kb max_used=105058Kb free=13249Kb\n",
      " bounds [0x00007f1a4d10b000, 0x00007f1a54494000, 0x00007f1a54494000]\n",
      "CodeHeap 'non-nmethods': size=9140Kb used=1610Kb max_used=5094Kb free=7529Kb\n",
      " bounds [0x00007f1a4c81e000, 0x00007f1a4cd5e000, 0x00007f1a4d10b000]\n",
      " total_blobs=38120 nmethods=37208 adapters=817\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "              stopped_count=1, restarted_count=0\n",
      " full_count=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: CodeHeap 'non-profiled nmethods' is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code heap size using -XX:NonProfiledCodeHeapSize=\n",
      "[Stage 35:===========================================>           (67 + 16) / 84]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2426121/232933071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"InterPro_annotations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/commons/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/commons/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/commons/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/commons/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/commons/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sf = small_df.select(\"InterPro_annotations\").distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e181e",
   "metadata": {},
   "source": [
    "#####  Pivot() It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fccb7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 23:03:01 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "22/07/08 23:20:48 WARN DAGScheduler: Broadcasting large task binary with size 25.1 MiB\n",
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "22/07/09 01:56:52 WARN DAGScheduler: Broadcasting large task binary with size 25.1 MiB\n",
      "22/07/09 02:08:03 WARN DAGScheduler: Broadcasting large task binary with size 25.1 MiB\n",
      "22/07/09 02:18:56 WARN DAGScheduler: Broadcasting large task binary with size 25.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivotDF = small_df.groupBy(\"Protein_accession\").pivot(\"InterPro_annotations\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae17b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a14c4",
   "metadata": {},
   "source": [
    "##### finding numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ed54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pivotDF.columns\n",
    "numericCols = [i for i in all_columns if ((i != \"Protein_accession\") & (i != \"InterPro_annotations\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01d9a6",
   "metadata": {},
   "source": [
    "##### chnageing the type of small features columns to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5abc9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.select(pivotDF.Protein_accession, \n",
    "                          (*(col(c).cast(IntegerType()).alias(c) for c in numericCols)),\n",
    "                          pivotDF.InterPro_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c74a0",
   "metadata": {},
   "source": [
    "###### left join of large dataframe(dataframe including protein and labels) and pivotDF(dataframe with small features in columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6226f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.join(largdff, on=['Protein_accession'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536203c",
   "metadata": {},
   "source": [
    "###### before doing random forest, all numeric columns are merged into a vector column using VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8738cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "ML_df = assembler.transform(pivotDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4390f75",
   "metadata": {},
   "source": [
    "###### label column is string, StringIndexer maps it to a label indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b076d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/09 02:46:32 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "22/07/09 02:46:35 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "22/07/09 09:23:48 WARN DAGScheduler: Broadcasting large task binary with size 25.1 MiB\n",
      "22/07/09 10:59:32 WARN DAGScheduler: Broadcasting large task binary with size 43.2 MiB\n",
      "22/07/09 11:00:34 WARN DAGScheduler: Broadcasting large task binary with size 43.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'InterPro_annotations', outputCol = 'labelIndex')\n",
    "ML_df = label_stringIdx.fit(ML_df).transform(ML_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9c12e",
   "metadata": {},
   "source": [
    "###### I kept just \"features\", \"labelIndex\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68954106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/09 11:09:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/07/09 11:10:24 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "22/07/09 11:10:29 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "22/07/09 15:34:48 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "22/07/09 15:59:58 WARN DAGScheduler: Broadcasting large task binary with size 46.4 MiB\n",
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|labelIndex|\n",
      "+--------------------+----------+\n",
      "| (5202,[3082],[1.0])|     745.0|\n",
      "|(5202,[875,902,27...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ML_df = ML_df.select(\"features\", \"labelIndex\")\n",
    "ML_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8a3b0",
   "metadata": {},
   "source": [
    "##### split data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd8fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = ML_df.randomSplit(weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38af702",
   "metadata": {},
   "source": [
    "# 2. ML inplementation\n",
    "###### applying random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc0ecfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labelIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c8173bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/09 17:27:49 WARN DAGScheduler: Broadcasting large task binary with size 14.9 MiB\n",
      "22/07/09 17:27:54 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "22/07/09 19:53:49 WARN DAGScheduler: Broadcasting large task binary with size 15.0 MiB\n",
      "22/07/09 22:10:43 WARN DAGScheduler: Broadcasting large task binary with size 24.1 MiB\n",
      "22/07/09 22:11:46 WARN DAGScheduler: Broadcasting large task binary with size 24.1 MiB\n",
      "22/07/09 22:40:42 WARN DAGScheduler: Broadcasting large task binary with size 24.2 MiB\n",
      "22/07/09 23:10:00 WARN DAGScheduler: Broadcasting large task binary with size 24.2 MiB\n",
      "22/07/09 23:51:23 WARN DAGScheduler: Broadcasting large task binary with size 25.7 MiB\n",
      "22/07/10 00:09:53 WARN DAGScheduler: Broadcasting large task binary with size 27.6 MiB\n",
      "22/07/10 00:28:11 WARN DAGScheduler: Broadcasting large task binary with size 29.4 MiB\n",
      "22/07/10 00:49:56 WARN DAGScheduler: Broadcasting large task binary with size 31.3 MiB\n",
      "22/07/10 00:56:56 WARN DAGScheduler: Broadcasting large task binary with size 1015.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb2766",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c474bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abd24ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/10 02:07:31 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "22/07/10 02:07:37 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "22/07/10 04:11:03 WARN DAGScheduler: Broadcasting large task binary with size 26.1 MiB\n",
      "22/07/10 06:19:36 WARN DAGScheduler: Broadcasting large task binary with size 52.1 MiB\n",
      "[Stage 91:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|labelIndex|prediction|\n",
      "+----------+----------+\n",
      "|    1029.0|       0.0|\n",
      "|       7.0|       0.0|\n",
      "|       6.0|       6.0|\n",
      "|      73.0|      63.0|\n",
      "|    1415.0|       0.0|\n",
      "|    1756.0|       0.0|\n",
      "|      10.0|       3.0|\n",
      "|      10.0|       3.0|\n",
      "|       3.0|       3.0|\n",
      "|      53.0|      53.0|\n",
      "+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"labelIndex\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bd51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/10 06:45:49 WARN DAGScheduler: Broadcasting large task binary with size 14.9 MiB\n",
      "22/07/10 06:45:53 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "22/07/10 08:31:32 WARN DAGScheduler: Broadcasting large task binary with size 15.0 MiB\n",
      "[Stage 94:======================================================> (81 + 3) / 84]\r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021c889",
   "metadata": {},
   "source": [
    "unfortunately, laptop turned off and I couldn't see the result; only next cell result is copied from another notebook(all steps are the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "512e4897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.14805097137616458\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %s\" % (accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
